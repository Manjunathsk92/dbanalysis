{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "routes = json.loads(open('/home/student/db/resources/trimmed_routes.json','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hawkins St\n",
      " Ballycullen Road\n",
      " Clongriffin\n",
      " Eden Quay\n"
     ]
    }
   ],
   "source": [
    "for v in routes['15']:\n",
    "    print (v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use the route to clongriffin as this is the longest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbanalysis import stop_tools\n",
    "import pandas as pd\n",
    "class BRModel():\n",
    "    \"\"\"\n",
    "    Big route model class\n",
    "    uses the distance of a stop from first stop on a route to compute predictions lalalala\n",
    "    MAPE and r2 scores are not as good as they were in the notebook\n",
    "    (we achieved 0.57 r2, and 7% MAPE on the time to complete the route)\n",
    "    Should look into this.\n",
    "    \"\"\"\n",
    "    def __init__ (self, route,variation,verbose=True,src='build',rgr='RandomForest',\\\n",
    "                mode='validate',features = ['base_time_dep','distance','rain','temp','day'],use_dummies=True):\n",
    "        \n",
    "        import json\n",
    "        self.regr_type = rgr\n",
    "        self.verbose = verbose\n",
    "        self.route = route\n",
    "        self.use_dummies = use_dummies\n",
    "        self.variation = variation\n",
    "        self.routes = json.loads(open('/home/student/dbanalysis/dbanalysis/resources/trimmed_routes.json').read())\n",
    "        self.features = features\n",
    "        self.route_array = self.routes[route][variation][1:]\n",
    "        del(self.routes)\n",
    "        if src == 'build':\n",
    "            if not self.can_be_modelled():\n",
    "                print('fuck')\n",
    "                raise ValueError ('Missing data for modelling this route')\n",
    "\n",
    "            self.gather_data()\n",
    "            self.preprocess()\n",
    "            if rgr == 'RandomForest':\n",
    "                from sklearn.ensemble import RandomForestRegressor as rf\n",
    "                self.rgr = rf()\n",
    "            elif rgr == 'Linear':\n",
    "                from sklearn.linear_model import LinearRegression as lr\n",
    "                self.rgr = lr(fit_intercept=True)\n",
    "            elif rgr == 'Neural':\n",
    "                from sklearn.neural_network import MLPRegressor as mlpr\n",
    "                self.rgr = mlpr(hidden_layer_sizes=(100,100),verbose=True)\n",
    "                from sklearn.preprocessing import normalize\n",
    "                \n",
    " \n",
    "            #if mode == 'validate':\n",
    "            #   if self.regr_type == 'Neural':\n",
    "                    #self.validate_neural()\n",
    "            #   else:\n",
    "                    #self.validate_model()\n",
    "            #lif mode == 'production':\n",
    "                #self.build_full_model()\n",
    "                #self.dump_model()\n",
    "    def validate_neural(self):\n",
    "        print('training regressor')\n",
    "        self.model = self.rgr.fit(self.train_X,self.train_Y)\n",
    "        preds = self.model.predict(self.test_1_X)\n",
    "        from sklearn import metrics\n",
    "        print('Validating...')\n",
    "        print(metrics.r2_score(self.test_1_Y,preds))\n",
    "        preds = self.model.predict(self.test_2_X)\n",
    "        print(metrics.r2_score(self.test_2_Y,preds))\n",
    "        print('calculated from norms-->')\n",
    "        preds = preds * self.test_2_norm\n",
    "        print(metrics.r2_score(self.test_2_real,preds))\n",
    "        preds = self.model.predict(self.test_3_X)\n",
    "        print(metrics.r2_score(self.test_3_Y,preds))\n",
    "    def validate_model(self):\n",
    "        self.data = self.data[self.data['traveltime']>0] \n",
    "        if self.verbose:\n",
    "            print('Validating model on all trips...\\n\\n')\n",
    "        \n",
    "        self.train = self.data[self.data['year']==2016]\n",
    "        self.test = self.data[self.data['year']==2017]\n",
    "        self.model = self.rgr.fit(self.train_X,self.train_Y)\n",
    "        preds = self.model.predict(self.test_1_X)\n",
    "        from sklearn import metrics\n",
    "        print('-----> Tested on all distances')\n",
    "        print('R2:', metrics.r2_score(self.test_1_Y,preds))\n",
    "        print('MAE:', metrics.mean_absolute_error(self.test_1_Y,preds)*self.norm[-1])\n",
    "        print('MAPE:', ((abs(self.test_1_Y-preds)/self.test_1_Y)*100).mean())\n",
    "        #add more options for testing eventually\n",
    "        print('Validating model on longest trip')\n",
    "        test2= self.test[self.test['distance']==self.test['distance'].max()]\n",
    "        if self.regr_type == 'Neural':\n",
    "            test2 = self.test[self.test['distance2']==self.test['distance2'].max()]\n",
    "        preds = self.model.predict(test2[self.features])\n",
    "        print('-----> ')\n",
    "        print('R2:', metrics.r2_score(test2['traveltime'],preds))\n",
    "        print('MAE:', metrics.mean_absolute_error(test2['traveltime'],preds)*self.norm[-1])\n",
    "        print('MAPE:', ((abs(test2['traveltime']-preds)/test2['traveltime'])*100).mean())\n",
    "        print('\\n\\n Validated on median trip -->')\n",
    "        test2= self.test[self.test['distance']==self.test['distance'].median()]\n",
    "        if self.regr_type == 'Neural':\n",
    "            test2 = self.test[self.test['distance2']==self.test['distance2'].median()]\n",
    "        \n",
    "        preds = self.model.predict(test2[self.features])\n",
    "        print('-----> ')\n",
    "        print('R2:', metrics.r2_score(test2['traveltime'],preds))\n",
    "        print('MAE:', metrics.mean_absolute_error(test2['traveltime'],preds)*self.norm[-1])\n",
    "        print('MAPE:', ((abs(test2['traveltime']-preds)/test2['traveltime'])*100).mean())\n",
    "        del(self.train)\n",
    "        del(self.test)\n",
    "        del(self.data)\n",
    "        del(test2)\n",
    "\n",
    "\n",
    "    def gather_data(self):\n",
    "        if self.verbose:\n",
    "            print('gathering data...')\n",
    "        from dbanalysis import stop_tools\n",
    "        arr = self.route_array\n",
    "        import os\n",
    "        to_concat = []\n",
    "        for i in range(len(arr)-1):\n",
    "            \n",
    "            data = stop_tools.get_stop_link(arr[i],arr[i+1])\n",
    "            to_concat.append(data)\n",
    "            del(data)\n",
    "        self.data = pd.concat(to_concat,axis=0)\n",
    "        del to_concat\n",
    "        \n",
    "\n",
    "    def preprocess(self):\n",
    "        if self.verbose:\n",
    "            print('Preprocessing data')\n",
    "        self.select_routes()\n",
    "        self.clean_1()\n",
    "        self.add_distances()\n",
    "        self.add_base_departure_time()\n",
    "        self.add_time_info()\n",
    "        \n",
    "        self.merge_weather()\n",
    "        #if self.use_dummies:\n",
    "        #    self.add_dummies()\n",
    "        #    self.features += self.dummy_features\n",
    "    def select_routes(self):\n",
    "        if self.verbose:\n",
    "            print('parsing routeids')\n",
    "        routeids = self.data['routeid'].unique()\n",
    "        valid_routeids = [r for r in routeids if r.split('_')[0] == self.route]\n",
    "        self.data = self.data[self.data['routeid'].isin(valid_routeids)]\n",
    "    def clean_1(self):\n",
    "        if self.verbose:\n",
    "            print('dropping null values')\n",
    "        self.data = self.data.dropna()\n",
    "        \n",
    "    def add_distances(self):\n",
    "        if self.verbose:\n",
    "            print('adding distances')\n",
    "        s_getter =stop_tools.stop_getter()\n",
    "        total_distance = 0\n",
    "        r = self.route_array\n",
    "        route_distances = {r[0]:0}\n",
    "        \n",
    "        for i in range(0, len(r)-1):\n",
    "            distance = s_getter.get_stop_distance(str(r[i]),str(r[i+1]))\n",
    "                \n",
    "            total_distance += distance\n",
    "            route_distances[r[i+1]]=total_distance\n",
    "        self.data['distance']=self.data['stopA'].apply(lambda x: route_distances[x])\n",
    "        del(s_getter)\n",
    "    def add_base_departure_time(self):\n",
    "        if self.verbose:\n",
    "            print('adding base departure times')\n",
    "       \n",
    "        keys= self.data[self.data['stopA']==self.route_array[0]]\n",
    "        keys['base_time_dep']=keys['actualtime_arr_from']\n",
    "        keys2=keys[['tripid','dayofservice','routeid','base_time_dep']]\n",
    "        self.data = pd.merge(self.data,keys2,on=['dayofservice','tripid','routeid'])\n",
    "        \n",
    "        self.data['traveltime']=self.data['actualtime_arr_from']-self.data['base_time_dep'] \n",
    "        #A number of rows have negative travel time. But its a really small number, so I guess they \n",
    "        #can go in the bin.\n",
    "        self.data = self.data[self.data['traveltime']>0]\n",
    "        del(keys)\n",
    "        del(keys2)\n",
    "    \n",
    "    def add_time_info(self):\n",
    "        if self.verbose:\n",
    "            print('adding time information')\n",
    "        time_format = \"%d-%b-%y %H:%M:%S\"\n",
    "        self.data['dt']=pd.to_datetime(self.data['dayofservice'],format=time_format)\n",
    "        self.data['day']=self.data['dt'].dt.dayofweek\n",
    "        self.data['month']=self.data['dt'].dt.month\n",
    "        self.data['hour'] = self.data['dt'].dt.hour\n",
    "        self.data['weekend']=self.data['day']>4\n",
    "        self.data['year']=self.data['dt'].dt.year\n",
    "        self.data['date'] = self.data['dt'].dt.date\n",
    "    def merge_weather(self,weather=None):\n",
    "        if self.verbose:\n",
    "            print('merging weather')\n",
    "        if weather == None:\n",
    "          \n",
    "            weather = pd.read_csv('/home/student/dbanalysis/dbanalysis/resources/cleanweather.csv').dropna()\n",
    "        weather['dt']=pd.to_datetime(weather['date'])\n",
    "        weather['hour']=weather['dt'].dt.hour\n",
    "        weather['date']=weather['dt'].dt.date\n",
    "        \n",
    "        self.data = pd.merge(self.data,weather,on=['date','hour'])\n",
    "        del(weather)\n",
    "\n",
    "    def add_dummies(self):\n",
    "        if self.verbose:\n",
    "            print('Making dummy features')\n",
    "        self.data = pd.get_dummies(self.data,columns=['day','month','hour'])\n",
    "        self.dummy_features = [col for col in self.data.columns\\\n",
    "                                if (col[0:3] == 'day' and col != 'dayofservice')\\\n",
    "                                or col[0:5] == 'month' or col[0:4] == 'hour']\n",
    "\n",
    "\n",
    "    def can_be_modelled(self):\n",
    "        if self.verbose:\n",
    "            print('Checking for data files')\n",
    "        import os\n",
    "        base_dir = '/data/stops/'\n",
    "        arr = self.route_array\n",
    "        for i in range(len(arr)-1):\n",
    "            \n",
    "            if os.path.exists(base_dir+str(arr[i])+'/'+str(arr[i+1])+'.csv'):\n",
    "                pass\n",
    "            else:\n",
    "                print('broken')\n",
    "                input()\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files\n",
      "gathering data...\n",
      "Preprocessing data\n",
      "parsing routeids\n",
      "dropping null values\n",
      "adding distances\n",
      "adding base departure times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding time information\n",
      "merging weather\n"
     ]
    }
   ],
   "source": [
    "r = BRModel('15',2,rgr='Neural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = r.data[r.data['year']==2016]\n",
    "test = r.data[r.data['year']==2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = ss()\n",
    "X = scaler_X.fit_transform(train[r.features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler_Y = ss()\n",
    "Y = scaler_Y.fit_transform(train['traveltime'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.02858333\n",
      "Iteration 2, loss = 0.02058436\n",
      "Iteration 3, loss = 0.01990401\n",
      "Iteration 4, loss = 0.01943416\n",
      "Iteration 5, loss = 0.01911647\n",
      "Iteration 6, loss = 0.01883062\n",
      "Iteration 7, loss = 0.01860965\n",
      "Iteration 8, loss = 0.01841233\n",
      "Iteration 9, loss = 0.01826826\n",
      "Iteration 10, loss = 0.01810811\n"
     ]
    }
   ],
   "source": [
    "model = r.rgr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = scaler_Y.inverse_transform(model.predict(X))\n",
    "((abs(train['traveltime']-preds)/train['traveltime'])*100).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "real_Y = test['traveltime']\n",
    "X = scaler_X.transform(test[r.features])\n",
    "preds = scaler_Y.inverse_transform(model.predict(X))\n",
    "print(((abs(real_Y-preds)/real_Y)*100).mean())\n",
    "print(metrics.r2_score(real_Y,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = sorted(test['distance'].unique())\n",
    "ds = []\n",
    "r2 = []\n",
    "mape = []\n",
    "for i in range(0,len(distances)-1):\n",
    "    distance1 = distances[i]\n",
    "    distance2 = distances[i+1]\n",
    "    X = test[(test['distance']>=distance1) & (test['distance'] < distance2)]\n",
    "    real_Y = X['traveltime']\n",
    "    X = scaler_X.transform(X[r.features])\n",
    "    preds =  scaler_Y.inverse_transform(model.predict(X))\n",
    "    ds.append(distance1)\n",
    "    from sklearn import metrics\n",
    "    r2.append(metrics.r2_score(real_Y,preds))\n",
    "    mape.append(((abs(real_Y - preds)/real_Y)*100).mean())\n",
    "    print(preds.min())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(ds,r2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(ds[5:],r2[5:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds,mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
